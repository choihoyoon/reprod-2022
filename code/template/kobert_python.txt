import transformers
import pandas as pd
import numpy as np
import urllib.request
import os
from tqdm import tqdm
import tensorflow as tf
from transformers import BertTokenizer, TFBertModel
from sklearn.model_selection import train_test_split

def convert_examples_to_features(self, examples, labels):
        input_ids, attention_masks, token_type_ids, data_labels = [], [], [], []

        for example, label in tqdm(zip(examples, labels), total=len(examples)):
            input_id = self.tokenizer.encode(example, max_length=self.max_seq_len, pad_to_max_length=True)
            padding_count = input_id.count(self.tokenizer.pad_token_id)
            attention_mask = [1] * (self.max_seq_len - padding_count) + [0] * padding_count
            token_type_id = [0] * self.max_seq_len

            assert len(input_id) == self.max_seq_len, "Error with input length {} vs {}".format(len(input_id),
                                                                                                self.max_seq_len)
            assert len(attention_mask) == self.max_seq_len, "Error with attention mask length {} vs {}".format(
                len(attention_mask), self.max_seq_len)
            assert len(token_type_id) == self.max_seq_len, "Error with token type length {} vs {}".format(
                len(token_type_id), self.max_seq_len)

            input_ids.append(input_id)
            attention_masks.append(attention_mask)
            token_type_ids.append(token_type_id)
            data_labels.append(label)

        input_ids = np.array(input_ids, dtype=int)
        attention_masks = np.array(attention_masks, dtype=int)
        token_type_ids = np.array(token_type_ids, dtype=int)

        data_labels = np.asarray(data_labels, dtype=np.int32)

        return (input_ids, attention_masks, token_type_ids), data_labels


class TFBertForSequenceClassification(tf.keras.Model):
    def __init__(self, model_name, activation, num_labels):
        super(TFBertForSequenceClassification, self).__init__()
        self.bert = TFBertModel.from_pretrained(model_name, from_pt=True)
        self.classifier = tf.keras.layers.Dense(num_labels,
                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(0.02),
                                                activation=activation,
                                                name='classifier')

    def call(self, inputs):
        input_ids, attention_mask, token_type_ids = inputs
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)
        cls_token = outputs[1]
        prediction = self.classifier(cls_token)

        return prediction


def main():
    df = pd.read_csv('../data/original.csv')

    target = df[['{{ target }}']]
    train_data, test_data = train_test_split(df, test_size={{ test_size }}, random_state=777, shuffle=True,
                                             stratify=target)
    target = train_data[['{{ target }}']]
    train_data, valid_data = train_test_split(train_data, test_size=0.2, random_state=777, shuffle=True,
                                              stratify=target)

    tokenizer = BertTokenizer.from_pretrained('klue/bert-base')

    max_seq_len = {{ max_length }}
    train_X, train_y = convert_examples_to_features(train_data['{{ input }}'], train_data['{{ target }}'],
                                                    max_seq_len=max_seq_len, tokenizer=tokenizer)
    valid_X, valid_y = convert_examples_to_features(valid_data['{{ input }}'], valid_data['{{ target }}'],
                                                    max_seq_len=max_seq_len, tokenizer=tokenizer)
    test_X, test_y = convert_examples_to_features(test_data['{{ input }}'], test_data['{{ target }}'],
                                                  max_seq_len=max_seq_len, tokenizer=tokenizer)

    if '{{ task }}' == 'binary':
        model = TFBertForSequenceClassification("klue/bert-base", activation='sigmoid', num_labels=1)
        loss = tf.keras.losses.BinaryCrossentropy()
    elif '{{ task }}' == 'multi-class':
        num_labels = df['{{ target }}'].nunique()
        model = TFBertForSequenceClassification("klue/bert-base", activation='softmax', num_labels=num_labels)
        loss = tf.keras.losses.SparseCategoricalCrossentropy()
    optimizer = tf.keras.optimizers.Adam(learning_rate={{ lr }})
    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])

    history = model.fit(train_X, train_y, epochs={{ n_epochs }}, batch_size= {{ batch_size }},
                        validation_data=(valid_X, valid_y))

    results = model.evaluate(test_X, test_y)
    print("test loss, test acc: ", results)


if __name__ == '__main__':
    main()
